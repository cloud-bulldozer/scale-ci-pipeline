- job:
    block-downstream: false
    block-upstream: false
    builders:
    - shell: |+
        set -o pipefail
        set -eux

        # get perf keys to access orchestration host and set ssh session options
        git clone https://${SSHKEY_TOKEN}@github.com/redhat-performance/perf-dept.git
        export PUBLIC_KEY=${WORKSPACE}/perf-dept/ssh_keys/id_rsa_pbench_ec2.pub
        export PRIVATE_KEY=${WORKSPACE}/perf-dept/ssh_keys/id_rsa_pbench_ec2
        export OPTIONS="-o StrictHostKeyChecking=no -o ServerAliveInterval=30 -o ServerAliveCountMax=1200"
        chmod 600 ${PRIVATE_KEY}

        # fetch the kubeconfig from the orchestration host
        echo "Fetching the  kubeconfig from the orchestration host"
        scp ${OPTIONS} -i ${PRIVATE_KEY} ${ORCHESTRATION_USER}@${ORCHESTRATION_HOST}:$HOME/.kube/config ${WORKSPACE}/kubeconfig
        export KUBECONFIG=${WORKSPACE}/kubeconfig

        # Create inventory File:
        echo "[orchestration]" > inventory
        echo "${ORCHESTRATION_HOST}" >> inventory

        export ANSIBLE_FORCE_COLOR=true
        ansible --version
        time ansible-playbook -vv -i inventory workloads/podvertical.yml

        # logging
        logs_counter=0
        logs_counter_limit=100
        oc logs --timestamps -n scale-ci-tooling -f job/scale-ci-podvertical
        while true; do
          logs_counter=$((logs_counter+1))
          if [[ $(oc get job -n scale-ci-tooling scale-ci-podvertical -o json | jq -e '.status.active==1') == "true" ]]; then
            if [[ $logs_counter -le $logs_counter_limit ]]; then
              echo "=================================================================================================================================================================="
              echo "Attempt $logs_counter to reconnect and fetch the controller pod logs"
              echo "=================================================================================================================================================================="
              oc logs --timestamps -n scale-ci-tooling -f job/scale-ci-podvertical
            else
              echo "Exceeded the retry limit trying to get the controller logs: $logs_counter_limit, exiting."
              exit 1
            fi
          else
            echo "Job completed"
            break
          fi
        done

        # status
        oc get job -n scale-ci-tooling scale-ci-podvertical -o json | jq -e '.status.succeeded==1'
    concurrent: true
    description: |
      PodVertical workload is used to validate and push the cluster limits for OpenShift. It loads max pods per namespace using cluster loader.
      This job is managed by https://github.com/openshift-scale/scale-ci-pipeline
    disabled: false
    name: ATS-SCALE-CI-PODVERTICAL
    node: scale-ci
    parameters:
    - string:
        default: ''
        description: this is a var to help idenfity cluster loaders results.
        name: SNAFU_USER
    - string:
        default: ''
        description: cluster name is a var to help idenfity cluster loaders results.(defaults to clustername found in machinset label)
        name: SNAFU_CLUSTER_NAME
    - string:
        default: ''
        description: Elasticsearch server host address (currently used by snafu), set to index results from cluster loader
        name: ES_HOST
    - string:
        default: ''
        description: Elasticsearch server port (currently used by snafu), set to index results from cluster loader
        name: ES_PORT
    - string:
        default: ''
        description: Elasticsearch server index prefix (currently used by snafu).(defaults to snafu through the workloads repo)
        name: ES_INDEX_PREFIX
    - string:
        default: ""
        description: Token to access private repo containing ssh keys.
        name: SSHKEY_TOKEN
    - string:
        default: ""
        description: The machine intended to run the oc commands and launch the workload.
        name: ORCHESTRATION_HOST
    - string:
        default: "root"
        description: The user for the Orchestration host.
        name: ORCHESTRATION_USER
    - string:
        default: "quay.io/openshift-scale/scale-ci-workload"
        description: Container image that runs the workload script.
        name: WORKLOAD_IMAGE
    - bool:
        default: false
        description: Enables/disables the node selector that places the workload job on the `pbench` node.
        name: WORKLOAD_JOB_NODE_SELECTOR
    - bool:
        default: false
        description: Enables/disables the toleration on the workload job to permit the `controller` taint.
        name: WORKLOAD_JOB_TAINT
    - bool:
        default: false
        description: Enables/disables running the workload pod as privileged.
        name: WORKLOAD_JOB_PRIVILEGED
    - string:
        default: "/root/.kube/config"
        description: Location of kubeconfig on orchestration host.
        name: KUBECONFIG_FILE
    - bool:
        default: false
        description: Enables/disables running the workload wrapped by pbench-user-benchmark. When enabled, pbench agents can then be enabled (ENABLE_PBENCH_AGENTS) for further instrumentation data and pbench-copy-results can be enabled (ENABLE_PBENCH_COPY) to export captured data for further analysis.
        name: PBENCH_INSTRUMENTATION
    - bool:
        default: false
        description: Enables/disables the collection of pbench data on the pbench agent pods. These pods are deployed by the tooling playbook.
        name: ENABLE_PBENCH_AGENTS
    - bool:
        default: false
        description: Enables/disables the copying of pbench data to a remote results server for further analysis.
        name: ENABLE_PBENCH_COPY
    - string:
        default: ""
        description: DNS address of the pbench results server.
        name: PBENCH_SERVER
    - string:
        default: ""
        description: Future use for pbench and prometheus scraper to place results into git repo that holds results data.
        name: SCALE_CI_RESULTS_TOKEN
    - string:
        default: 0
        description: Number of retries for Ansible to poll if the workload job has completed. Poll attempts delay 10s between polls with some additional time taken for each polling action depending on the orchestration host setup.
        name: JOB_COMPLETION_POLL_ATTEMPTS
    - string:
        default: "podvertical"
        description: Test to prefix the pbench results.
        name: PODVERTICAL_TEST_PREFIX
    - bool:
        default: true
        description: Enables/disables cluster loader cleanup of this workload on completion.
        name: PODVERTICAL_CLEANUP
    - string:
        default: "podvertical"
        description: Basename used by cluster loader for the project(s) it creates.
        name: PODVERTICAL_BASENAME
    - string:
        default: 1000
        description: "Maximum number of pods per namespace that PodVertical will create across the cluster."
        name: PODVERTICAL_MAXPODS
    - string:
        default: "gcr.io/google_containers/pause-amd64:3.0"
        description: Pod image to use for pods that are nodevertical pods.
        name: PODVERTICAL_POD_IMAGE
    - string:
        default: 500
        description: Number of pods for cluster loader will create before waiting for pods to become running.
        name: PODVERTICAL_STEPSIZE
    - string:
        default: 30
        description: Period of time (in seconds) for cluster loader to pause after creating pods and waiting for them to be running.
        name: PODVERTICAL_PAUSE
    - string:
        default: 600
        description: Period of time (in seconds) that cluster loader will wait for pods to come up to "Running" state per tuningset before sleeping PODVERTICAL_PAUSE. The tuningset is determined by PODVERTICAL_STEPSIZE and PODVERTICAL_PAUSE thus if you have a very large stepsize you will need a greater period of time to allow the Pods to come to "Running" state. This value prevents waiting infinitely for Pods that would otherwise never come up.
        name: PODVERTICAL_TS_TIMEOUT
    - string:
        default: 600
        description: Pass/fail criteria. Value to determine if PodVertical workload executed in duration expected.
        name: EXPECTED_PODVERTICAL_DURATION
    project-type: freestyle
    properties:
    - raw:
        xml: |
          <hudson.plugins.disk__usage.DiskUsageProperty plugin="disk-usage@0.28" />
    - raw:
        xml: |
          <com.dabsquared.gitlabjenkins.connection.GitLabConnectionProperty plugin="gitlab-plugin@1.5.3">
          <gitLabConnection />
          </com.dabsquared.gitlabjenkins.connection.GitLabConnectionProperty>
    - raw:
        xml: |
          <org.jenkinsci.plugins.ZMQEventPublisher.HudsonNotificationProperty plugin="zmq-event-publisher@0.0.5">
          <enabled>false</enabled>
          </org.jenkinsci.plugins.ZMQEventPublisher.HudsonNotificationProperty>
    - raw:
        xml: |
          <com.synopsys.arc.jenkins.plugins.ownership.jobs.JobOwnerJobProperty plugin="ownership@0.11.0">
          <ownership>
          <ownershipEnabled>true</ownershipEnabled>
          <primaryOwnerId>nelluri</primaryOwnerId>
          <coownersIds class="sorted-set" />
          </ownership>
          </com.synopsys.arc.jenkins.plugins.ownership.jobs.JobOwnerJobProperty>
    - raw:
        xml: |
          <com.sonyericsson.rebuild.RebuildSettings plugin="rebuild@1.27">
          <autoRebuild>false</autoRebuild>
          <rebuildDisabled>false</rebuildDisabled>
          </com.sonyericsson.rebuild.RebuildSettings>
    - raw:
        xml: |
          <hudson.plugins.throttleconcurrents.ThrottleJobProperty plugin="throttle-concurrents@2.0.1">
          <maxConcurrentPerNode>0</maxConcurrentPerNode>
          <maxConcurrentTotal>0</maxConcurrentTotal>
          <categories class="java.util.concurrent.CopyOnWriteArrayList" />
          <throttleEnabled>false</throttleEnabled>
          <throttleOption>project</throttleOption>
          <limitOneJobWithMatchingParams>false</limitOneJobWithMatchingParams>
          <paramsToUseForLimit />
          </hudson.plugins.throttleconcurrents.ThrottleJobProperty>
    publishers: []
    scm:
    - git:
        branches:
        - '*/master'
        url: https://github.com/openshift-scale/workloads.git
    triggers: []
    wrappers:
    - workspace-cleanup:
        dirmatch: false
    - ansicolor:
        colormap: xterm
