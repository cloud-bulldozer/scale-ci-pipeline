- job:
    block-downstream: false
    block-upstream: false
    builders:
    - shell: |+
        set -o pipefail
        set -eux

        # get perf keys to access orchestration host and set ssh session options
        git clone https://${SSHKEY_TOKEN}@github.com/redhat-performance/perf-dept.git
        export PUBLIC_KEY=${WORKSPACE}/perf-dept/ssh_keys/id_rsa_pbench_ec2.pub
        export PRIVATE_KEY=${WORKSPACE}/perf-dept/ssh_keys/id_rsa_pbench_ec2
        export OPTIONS="-o StrictHostKeyChecking=no -o ServerAliveInterval=30 -o ServerAliveCountMax=1200"
        chmod 600 ${PRIVATE_KEY}

        # fetch the kubeconfig from the orchestration host
        echo "Fetching the  kubeconfig from the orchestration host"
        scp ${OPTIONS} -i ${PRIVATE_KEY} ${ORCHESTRATION_USER}@${ORCHESTRATION_HOST}:$HOME/.kube/config ${WORKSPACE}/kubeconfig
        export KUBECONFIG=${WORKSPACE}/kubeconfig

        # Create inventory File:
        echo "[orchestration]" > inventory
        echo "${ORCHESTRATION_HOST}" >> inventory

        export ANSIBLE_FORCE_COLOR=true
        ansible --version
        time ansible-playbook -vv -i inventory workloads/nodevertical.yml

        # logging
        logs_counter=0
        logs_counter_limit=100
        oc logs --timestamps -n scale-ci-tooling -f job/scale-ci-nodevertical
        while true; do
          logs_counter=$((logs_counter+1))
          if [[ $(oc get job -n scale-ci-tooling scale-ci-nodevertical -o json | jq -e '.status.active==1') == "true" ]]; then
            if [[ $logs_counter -le $logs_counter_limit ]]; then
              echo "=================================================================================================================================================================="
              echo "Attempt $logs_counter to reconnect and fetch the controller pod logs"
              echo "=================================================================================================================================================================="
              oc logs --timestamps -n scale-ci-tooling -f job/scale-ci-nodevertical
            else
              echo "Exceeded the retry limit trying to get the controller logs: $logs_counter_limit, exiting."
              exit 1
            fi
          else
            echo "Job completed"
            break
          fi
        done

        # status
        oc get job -n scale-ci-tooling scale-ci-nodevertical -o json | jq -e '.status.succeeded==1'
    concurrent: true
    description: |
      NodeVertical workload is used to measure kubelet density for OpenShift. This means NodeVertical loads several nodes to max pods using cluster loader.
      This job is managed by https://github.com/openshift-scale/scale-ci-pipeline
    disabled: false
    name: ATS-SCALE-CI-NODEVERTICAL
    node: scale-ci
    parameters:
    - string:
        default: ''
        description: this is a var to help idenfity cluster loaders results.
        name: SNAFU_USER
    - string:
        default: ''
        description: cluster name is a var to help idenfity cluster loaders results.(defaults to clustername found in machinset label)
        name: SNAFU_CLUSTER_NAME
    - string:
        default: ''
        description: Elasticsearch server host address (currently used by snafu), set to index results from cluster loader
        name: ES_HOST
    - string:
        default: ''
        description: Elasticsearch server port (currently used by snafu), set to index results from cluster loader
        name: ES_PORT
    - string:
        default: ''
        description: Elasticsearch server index prefix (currently used by snafu). (defaults to snafu through the workloads repo)
        name: ES_INDEX_PREFIX
    - string:
        default: ""
        description: Token to access private repo containing ssh keys.
        name: SSHKEY_TOKEN
    - string:
        default: ""
        description: The machine intended to run the oc commands and launch the workload.
        name: ORCHESTRATION_HOST
    - string:
        default: "root"
        description: The user for the Orchestration host.
        name: ORCHESTRATION_USER
    - string:
        default: "quay.io/openshift-scale/scale-ci-workload"
        description: Container image that runs the workload script.
        name: WORKLOAD_IMAGE
    - bool:
        default: false
        description: Enables/disables the node selector that places the workload job on the `pbench` node.
        name: WORKLOAD_JOB_NODE_SELECTOR
    - bool:
        default: false
        description: Enables/disables the toleration on the workload job to permit the `controller` taint.
        name: WORKLOAD_JOB_TAINT
    - bool:
        default: false
        description: Enables/disables running the workload pod as privileged.
        name: WORKLOAD_JOB_PRIVILEGED
    - string:
        default: "/root/.kube/config"
        description: Location of kubeconfig on orchestration host.
        name: KUBECONFIG_FILE
    - bool:
        default: false
        description: Enables/disables running the workload wrapped by pbench-user-benchmark. When enabled, pbench agents can then be enabled (ENABLE_PBENCH_AGENTS) for further instrumentation data and pbench-copy-results can be enabled (ENABLE_PBENCH_COPY) to export captured data for further analysis.
        name: PBENCH_INSTRUMENTATION
    - bool:
        default: false
        description: Enables/disables the collection of pbench data on the pbench agent pods. These pods are deployed by the tooling playbook.
        name: ENABLE_PBENCH_AGENTS
    - bool:
        default: false
        description: Enables/disables the copying of pbench data to a remote results server for further analysis.
        name: ENABLE_PBENCH_COPY
    - string:
        default: ""
        description: DNS address of the pbench results server.
        name: PBENCH_SERVER
    - string:
        default: ""
        description: Future use for pbench and prometheus scraper to place results into git repo that holds results data.
        name: SCALE_CI_RESULTS_TOKEN
    - string:
        default: 0
        description: Number of retries for Ansible to poll if the workload job has completed. Poll attempts delay 10s between polls with some additional time taken for each polling action depending on the orchestration host setup.
        name: JOB_COMPLETION_POLL_ATTEMPTS
    - string:
        default: 4
        description: Number of nodes to apply the nodevertical label to. This isolates the NodeVertical Pods to achieve kubelet density. You will have to adjust the value here for clusters < 5 nodes.
        name: NODEVERTICAL_NODE_COUNT
    - string:
        default: "nodevertical"
        description: Test to prefix the pbench results.
        name: NODEVERTICAL_TEST_PREFIX
    - bool:
        default: true
        description: Enables/disables cluster loader cleanup of this workload on completion.
        name: NODEVERTICAL_CLEANUP
    - string:
        default: "nodevertical"
        description: Basename used by cluster loader for the project(s) it creates.
        name: NODEVERTICAL_BASENAME
    - string:
        default: 1000
        description: "Maximum number of pods that NodeVertical will create across the nodes it has labeled. The number of pods will be calculated against this value by subtracting the non-terminated pod count across the labeled nodes. Example: 4 nodes each with 10 pods, Max pods set to 250 per node and NODEVERTICAL_MAXPODS is set to 1000, then nodevertical will spawn 960 pods ((4 x 250) - (4 * 10))."
        name: NODEVERTICAL_MAXPODS
    - string:
        default: "gcr.io/google_containers/pause-amd64:3.0"
        description: Pod image to use for pods that are nodevertical pods.
        name: NODEVERTICAL_POD_IMAGE
    - string:
        default: 50
        description: Number of pods for cluster loader will create before waiting for pods to become running.
        name: NODEVERTICAL_STEPSIZE
    - string:
        default: 60
        description: Period of time (in seconds) for cluster loader to pause after creating pods and waiting for them to be running.
        name: NODEVERTICAL_PAUSE
    - string:
        default: 180
        description: Period of time (in seconds) that cluster loader will wait for pods to come up to "Running" state per tuningset before sleeping NODEVERTICAL_PAUSE. The tuningset is determined by NODEVERTICAL_STEPSIZE and NODEVERTICAL_PAUSE thus if you have a very large stepsize you will need a greater period of time to allow the Pods to come to "Running" state. This value prevents waiting infinitely for Pods that would otherwise never come up.
        name: NODEVERTICAL_TS_TIMEOUT
    - string:
        default: 600
        description: Pass/fail criteria. Value to determine if NodeVertical workload executed in duration expected.
        name: EXPECTED_NODEVERTICAL_DURATION
    - bool:
        default: false
        description: Trigger nodevertical heavy workload.
        name: NODEVERTICAL_HEAVY
    - string:
        default: '/ready'
        description: Readiness probe endpoint for the application deployed by the heavy nodevertical
        name: NODEVERTICAL_HEAVY_PROBE_ENDPOINT
    - string:
        default: 30
        description: Readiness probe period for the application deployed by the heavy nodevertical.
        name: NODEVERTICAL_HEAVY_PROBE_PERIOD

    project-type: freestyle
    properties:
    - raw:
        xml: |
          <hudson.plugins.disk__usage.DiskUsageProperty plugin="disk-usage@0.28" />
    - raw:
        xml: |
          <com.dabsquared.gitlabjenkins.connection.GitLabConnectionProperty plugin="gitlab-plugin@1.5.3">
          <gitLabConnection />
          </com.dabsquared.gitlabjenkins.connection.GitLabConnectionProperty>
    - raw:
        xml: |
          <org.jenkinsci.plugins.ZMQEventPublisher.HudsonNotificationProperty plugin="zmq-event-publisher@0.0.5">
          <enabled>false</enabled>
          </org.jenkinsci.plugins.ZMQEventPublisher.HudsonNotificationProperty>
    - raw:
        xml: |
          <com.synopsys.arc.jenkins.plugins.ownership.jobs.JobOwnerJobProperty plugin="ownership@0.11.0">
          <ownership>
          <ownershipEnabled>true</ownershipEnabled>
          <primaryOwnerId>nelluri</primaryOwnerId>
          <coownersIds class="sorted-set" />
          </ownership>
          </com.synopsys.arc.jenkins.plugins.ownership.jobs.JobOwnerJobProperty>
    - raw:
        xml: |
          <com.sonyericsson.rebuild.RebuildSettings plugin="rebuild@1.27">
          <autoRebuild>false</autoRebuild>
          <rebuildDisabled>false</rebuildDisabled>
          </com.sonyericsson.rebuild.RebuildSettings>
    - raw:
        xml: |
          <hudson.plugins.throttleconcurrents.ThrottleJobProperty plugin="throttle-concurrents@2.0.1">
          <maxConcurrentPerNode>0</maxConcurrentPerNode>
          <maxConcurrentTotal>0</maxConcurrentTotal>
          <categories class="java.util.concurrent.CopyOnWriteArrayList" />
          <throttleEnabled>false</throttleEnabled>
          <throttleOption>project</throttleOption>
          <limitOneJobWithMatchingParams>false</limitOneJobWithMatchingParams>
          <paramsToUseForLimit />
          </hudson.plugins.throttleconcurrents.ThrottleJobProperty>
    publishers: []
    scm:
    - git:
        branches:
        - '*/master'
        url: https://github.com/openshift-scale/workloads.git
    triggers: []
    wrappers:
    - workspace-cleanup:
        dirmatch: false
    - ansicolor:
        colormap: xterm
